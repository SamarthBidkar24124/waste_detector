{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47dd3f-4dca-4db0-a68d-800b30f59fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "data_train_path = r\"C:\\Users\\mohan\\Downloads\\model training SAMARTH\\model training\\train\"\n",
    "data_val_path = r\"C:\\Users\\mohan\\Downloads\\model training SAMARTH\\model training\\valid\"\n",
    "data_test_path = r\"C:\\Users\\mohan\\Downloads\\model training SAMARTH\\model training\\test\"\n",
    "\n",
    "# Params\n",
    "img_width, img_height = 180, 180\n",
    "img_size = (img_width, img_height)\n",
    "batch_size = 32  # increased batch size for better batch statistics\n",
    "num_classes = 1  # binary classification (wet/dry)\n",
    "\n",
    "# Data augmentation - increased a bit for better generalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True  # added vertical flip since waste images may vary\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Data loaders\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    data_train_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data = val_test_datagen.flow_from_directory(\n",
    "    data_val_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_data = val_test_datagen.flow_from_directory(\n",
    "    data_test_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load base MobileNetV2 model pretrained on ImageNet\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Unfreeze last few layers of the base model for fine-tuning (say last 20 layers)\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Build your model with L2 regularization\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.4),  # increased dropout\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile with a lower learning rate to avoid large jumps in fine-tuning\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\"best_model.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Train\n",
    "epochs = 50  # more epochs with early stopping\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Plot Loss & Accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', color='orange')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', color='green')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='red')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"\\nâœ… Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
